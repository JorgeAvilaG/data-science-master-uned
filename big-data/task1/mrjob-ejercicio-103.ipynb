{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1.2\n",
    "## Jorge Pablo Ávila Gómez "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1.3: Mejorando el país con mejores clientes.\n",
    "Partiendo del código implementado para el ejercicio anterior, mejóralo para que,\n",
    "en el caso de que haya más de un país empatado con el mayor número de\n",
    "buenos clientes, se devuelvan todos esos países. Utilizando los ficheros de datos\n",
    "sobre países y clientes vistos anteriormente, la salida de este programa\n",
    "MapReduce debería ser la que se muestra en la Figura 3. Hay\n",
    "que tener en\n",
    "cuenta que la entrada del reducer es una clave y un generator que contiene los\n",
    "valores que comparten la misma clave. Este ejercicio tiene una puntuación de\n",
    "hasta 4 puntos sobre 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diseño del programa MapReduce\n",
    "#### Limpieza de los datos\n",
    "En la primera parte del ejercicio se realiza la misma limpieza de datos realizada en el Ejercicio 1.1\n",
    "\n",
    "Se elimina la cabezera del documento countries y se limpian las columnas quitando la coma y el entrecomillado:\n",
    "\n",
    "`\"Bolivia, Plurinational State of\" -> Plurinational State of Bolivia`\n",
    "#### Diseño MapReduce\n",
    "El ejercicio se resuelto usando dos pasos, en el primero se ejecuta un mapper y un reducer, y en el segundo paso un reducer.\n",
    "- Map. (`mapper_find_bueno`) El código de este mapper es idéntico al del ejercicio 1.1 y 1.2. El map se usa para leer los dos documentos y extraer la información necesaria.\n",
    "Se necesita extraer la columna nombre del pais del documentro countries y la columna valoración == 'bueno' del documento clients. Las dos tablas se encuentran enlazadas por la columna country2digit.\n",
    "\n",
    "Para el documento countries se separa en dos columnas, nombre del pais y country2digit. Se envia como clave la columna country2digit y como valor una lista formada por la letra 'A' y el nombre del pais.\n",
    "\n",
    "Para el documento clients se separa en tres columnas, nombre del cliente, valoración y country2digit. Aquí es importante que filtremos las columnas en las que la valoración es 'bueno'. Enviamos como clave country2digit, que es la columna que nos sirve para relacionar las dos tablas, y como valor enviamos una lista formada por la letra 'B' y como segundo elemento el número 1. Indicando que para ese pais hemos encontrado al menos 1 valoración buena.\n",
    "\n",
    "Se utiliza las etiquetas 'A' y 'B' junto con la opción: \n",
    "```python \n",
    "SORT_VALUES = True```\n",
    "Para que al reducer los valores llegen ordenados. Como sabemos, habrá un reducer para cada country2digit, es decir para cada país. Y los valores vendrán ordenados primero con la etiqueta 'A', solo habrá una y corresponde al documento countries con el nombre completo del pais, y luego todos los valores con la etiqueta 'B', procedentes del documento clients indicando que se ha encontrado una valoración buena.\n",
    "\n",
    "- Reduce. (`reducer_count_bueno`) La función reducer va a contar el numero de valoraciones positivas que le llega de cada país. El código es idéntico al del ejercicio 1.2.\n",
    "\n",
    "Para cada country2digit primero recibirá una lista con la etiqueda 'A' y el nombre completo del país. Luego recibirá un número de valores con la etiqueta 'B'. El reducer va sumando cuantas 'B' le llegan correspodiéndose cada una con una valoración buena.\n",
    "\n",
    "El reducer devuelve siempre la clave `None`, para que todos los valores se dirijan al mismo reducer final. Como valores devuelve una lista, el primer elemento es el número de valoraciones positivas y el segundo elemento el nombre del país.\n",
    "\n",
    "- Reduce. (`reducer_sort_countries`) Este reducer se encarga de devolver todos los países con el mayor número de valoraciones buenas.\n",
    "\n",
    "A este reducer le llegan todos los valores enviados por el reducer anterior (valores con clave `None`) que además están ordenados (de menor a mayor) por la línea SORT_VALUES = True.\n",
    "\n",
    "Como primer paso los valores se pasan a una lista y se invierte la lista, así los tenemos ordenados de mayor a menor.\n",
    "\n",
    "Para cada elemento de la lista devolvemos el primero (el pais con más valoraciones buenas) y los siguientes elementos que tengan el mismo número de valoraciones. \n",
    "Cuando nos encontramos con un elemento con un número de valoraciones menor terminamos la función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/tp1/ejercicio103\r\n"
     ]
    }
   ],
   "source": [
    "# Se crea la carpeta ejercicio103 donde se van a almacenar los documentos que se generen\n",
    "! mkdir -p tp1/ejercicio103\n",
    "import os\n",
    "os.chdir(\"/media/notebooks/tp1/ejercicio103\")\n",
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los ficheros countries.csv y clients.csv deben estar descargados en la carpeta /media/notebooks/tp1/ejercicio103\n",
    "! cp /media/notebooks/countries.csv /media/notebooks/tp1/ejercicio103/\n",
    "! cp /media/notebooks/clients.csv /media/notebooks/tp1/ejercicio103/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Bolivia, Plurinational State of\",BO\n",
      "\n",
      "Plurinational State of Bolivia,BO\n",
      "\n",
      "\"Bonaire, Sint Eustatius and Saba\",BQ\n",
      "\n",
      "Sint Eustatius and Saba Bonaire,BQ\n",
      "\n",
      "\"Congo, the Democratic Republic of the\",CD\n",
      "\n",
      "the Democratic Republic of the Congo,CD\n",
      "\n",
      "\"Iran, Islamic Republic of\",IR\n",
      "\n",
      "Islamic Republic of Iran,IR\n",
      "\n",
      "\"Korea, Democratic People's Republic of\",KP\n",
      "\n",
      "Democratic People's Republic of Korea,KP\n",
      "\n",
      "\"Korea, Republic of\",KR\n",
      "\n",
      "Republic of Korea,KR\n",
      "\n",
      "\"Macedonia, the Former Yugoslav Republic of\",MK\n",
      "\n",
      "the Former Yugoslav Republic of Macedonia,MK\n",
      "\n",
      "\"Micronesia, Federated States of\",FM\n",
      "\n",
      "Federated States of Micronesia,FM\n",
      "\n",
      "\"Moldova, Republic of\",MD\n",
      "\n",
      "Republic of Moldova,MD\n",
      "\n",
      "\"Palestine, State of\",PS\n",
      "\n",
      "State of Palestine,PS\n",
      "\n",
      "\"Saint Helena, Ascension and Tristan da Cunha\",SH\n",
      "\n",
      "Ascension and Tristan da Cunha Saint Helena,SH\n",
      "\n",
      "\"Taiwan, Province of China\",TW\n",
      "\n",
      "Province of China Taiwan,TW\n",
      "\n",
      "\"Tanzania, United Republic of\",TZ\n",
      "\n",
      "United Republic of Tanzania,TZ\n",
      "\n",
      "\"Venezuela, Bolivarian Republic of\",VE\n",
      "\n",
      "Bolivarian Republic of Venezuela,VE\n",
      "\n",
      "\"Virgin Islands, British\",VG\n",
      "\n",
      "British Virgin Islands,VG\n",
      "\n",
      "\"Virgin Islands, U.S.\",VI\n",
      "\n",
      "U.S. Virgin Islands,VI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Limpieza de countries.csv\n",
    "\n",
    "with open(\"countries.csv\",'r') as f, open(\"countries_cleaned.csv\",'w') as f1:\n",
    "    next(f) # salta el encabezado\n",
    "    for line in f:\n",
    "        if '''\"''' in line: # Los paises con el problema de la coma estan entrecomillados\n",
    "            loc = line.find(',') # Elimina la primera coma para no confundir con la separacion entre columnas\n",
    "            new_line = line[loc+2:-5] + ' ' + line[1:loc] + line[-4:] # Reordena el nombre del pais\n",
    "            f1.write(new_line)\n",
    "            print(line)\n",
    "            print(new_line)\n",
    "        else:\n",
    "            f1.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob-ejercicio103.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob-ejercicio103.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "\n",
    "    SORT_VALUES = True\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper_find_bueno,                \n",
    "                       reducer=self.reducer_count_bueno),\n",
    "                MRStep(reducer=self.reducer_sort_countries),\n",
    "               ]\n",
    "    \n",
    "    def mapper_find_bueno(self, _, line):\n",
    "        splits = line.rstrip(\"\\n\").split(\",\")\n",
    "        if len(splits) == 2: # datos de paises\n",
    "            symbol = 'A'     # diferencia datos de paises de clientes\n",
    "            country2digit = splits[1]\n",
    "            yield country2digit, [symbol, splits[0]] #valor es el simbolo A y el nombre del pais completo\n",
    "        else: #  datos de personas\n",
    "            symbol = 'B' \n",
    "            country2digit = splits[2]\n",
    "            if splits[1] == 'bueno': # seleccionamos solo los clientes con una valoracion buena\n",
    "                yield country2digit, [symbol, 1] # enviamos solo un valor cuando hemos encontrado una val buena \n",
    "    \n",
    "    def reducer_count_bueno(self, key, values):\n",
    "        for value in values:\n",
    "            if value[0] == 'A': # dato de pais\n",
    "                country = value[1] # nombre del pais\n",
    "                counter = 0\n",
    "            if value[0] == 'B': # dato de persona\n",
    "                counter +=1 # sumamos 1 por cada valoración buena\n",
    "        else:\n",
    "            if counter > 0: # solo enviamos resultados hay al menos 1 valoración buena\n",
    "                yield None, [counter, country] # clave None para que llegue al mismo reducer final     \n",
    "\n",
    "    def reducer_sort_countries(self, key, values):      \n",
    "        countries = reversed(list(values)) # invierte los valores para que esten ordenados de mayor a menor\n",
    "        counter = -1\n",
    "        for country in countries:\n",
    "            if country[0] >= counter: # el primer elemento de la lista y todos los que tienen el mismo valor\n",
    "                yield country\n",
    "                counter = country[0] \n",
    "            else: # cuando encontramos un elemento que tiene un valor menor no necesitamos seguir ejecutando la f\n",
    "                break    \n",
    "                \n",
    "if __name__ == '__main__':\n",
    "    MRJoin.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primero ejecutamos el código en local:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/mrjob-ejercicio103.root.20201030.165700.322089\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/mrjob-ejercicio103.root.20201030.165700.322089/output\n",
      "Streaming final output from /tmp/mrjob-ejercicio103.root.20201030.165700.322089/output...\n",
      "Removing temp directory /tmp/mrjob-ejercicio103.root.20201030.165700.322089...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio103.py /media/notebooks/tp1/ejercicio103/countries_cleaned.csv  \\\n",
    "/media/notebooks/tp1/ejercicio103/clients.csv > ouputlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t\"Spain\"\r\n",
      "3\t\"Guam\"\r\n"
     ]
    }
   ],
   "source": [
    "! tail ouputlocal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecutamos el código en hadoop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/tmp/mrjoin/ejercicio103': File exists\n",
      "put: `/tmp/mrjoin/ejercicio103/countries_cleaned.csv': File exists\n",
      "put: `/tmp/mrjoin/ejercicio103/clients.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -mkdir /tmp/mrjoin/ejercicio103\n",
    "! hdfs dfs -put /media/notebooks/tp1/ejercicio103/countries_cleaned.csv  /tmp/mrjoin/ejercicio103\n",
    "! hdfs dfs -put /media/notebooks/tp1/ejercicio103/clients.csv  /tmp/mrjoin/ejercicio103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/carpeta/mrjob-ejercicio103/_SUCCESS\r\n",
      "Deleted /tmp/carpeta/mrjob-ejercicio103/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm /tmp/carpeta/mrjob-ejercicio103/*\n",
    "! hdfs dfs -rmdir /tmp/carpeta/mrjob-ejercicio103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/lib/hadoop/bin...\n",
      "Found hadoop binary: /usr/lib/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/mrjob-ejercicio103.root.20201030.165713.088786\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/mrjob-ejercicio103.root.20201030.165713.088786/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/mrjob-ejercicio103.root.20201030.165713.088786/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob2395509284175184545.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.19.0.4:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.19.0.4:8032\n",
      "  Total input paths to process : 2\n",
      "  number of splits:3\n",
      "  Submitting tokens for job: job_1604063155650_0015\n",
      "  Submitted application application_1604063155650_0015\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1604063155650_0015/\n",
      "  Running job: job_1604063155650_0015\n",
      "  Job job_1604063155650_0015 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1604063155650_0015 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mrjob-ejercicio103.root.20201030.165713.088786/step-output/0000\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6363\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=283\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=7596\n",
      "\t\tFILE: Number of bytes written=611125\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6707\n",
      "\t\tHDFS: Number of bytes written=283\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=11528192\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1580032\n",
      "\t\tTotal time spent by all map tasks (ms)=11258\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11258\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1543\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1543\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=11258\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1543\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1400\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=359\n",
      "\t\tInput split bytes=344\n",
      "\t\tMap input records=299\n",
      "\t\tMap output bytes=7060\n",
      "\t\tMap output materialized bytes=7608\n",
      "\t\tMap output records=265\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tPhysical memory (bytes) snapshot=1094107136\n",
      "\t\tReduce input groups=261\n",
      "\t\tReduce input records=265\n",
      "\t\tReduce output records=12\n",
      "\t\tReduce shuffle bytes=7608\n",
      "\t\tShuffled Maps =3\n",
      "\t\tSpilled Records=530\n",
      "\t\tTotal committed heap usage (bytes)=1215299584\n",
      "\t\tVirtual memory (bytes) snapshot=10450165760\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob2598284244894597724.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.19.0.4:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.19.0.4:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1604063155650_0016\n",
      "  Submitted application application_1604063155650_0016\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1604063155650_0016/\n",
      "  Running job: job_1604063155650_0016\n",
      "  Job job_1604063155650_0016 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1604063155650_0016 completed successfully\n",
      "  Output directory: hdfs:///tmp/carpeta/mrjob-ejercicio103\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=425\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=19\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=325\n",
      "\t\tFILE: Number of bytes written=447265\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=759\n",
      "\t\tHDFS: Number of bytes written=19\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4313088\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1572864\n",
      "\t\tTotal time spent by all map tasks (ms)=4212\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4212\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1536\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1536\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4212\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1536\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1040\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=141\n",
      "\t\tInput split bytes=334\n",
      "\t\tMap input records=12\n",
      "\t\tMap output bytes=295\n",
      "\t\tMap output materialized bytes=331\n",
      "\t\tMap output records=12\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=787800064\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce input records=12\n",
      "\t\tReduce output records=2\n",
      "\t\tReduce shuffle bytes=331\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=24\n",
      "\t\tTotal committed heap usage (bytes)=841482240\n",
      "\t\tVirtual memory (bytes) snapshot=7841431552\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///tmp/carpeta/mrjob-ejercicio103\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/mrjob-ejercicio103.root.20201030.165713.088786...\n",
      "Removing temp directory /tmp/mrjob-ejercicio103.root.20201030.165713.088786...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio103.py hdfs:///tmp/mrjoin/ejercicio103/* -r hadoop --python-bin /opt/anaconda/bin/python3.7 \\\n",
    "--output-dir hdfs:///tmp/carpeta/mrjob-ejercicio103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t\"Spain\"\r\n",
      "3\t\"Guam\"\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -tail /tmp/carpeta/mrjob-ejercicio103/part-00000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
