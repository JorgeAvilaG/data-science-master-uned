{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1.1\n",
    "## Jorge Pablo Ávila Gómez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1.1: Contador de clientes valorados por países.\n",
    "Partiendo de los ficheros de datos de países y clientes y del código visto en el\n",
    "ejemplo mrjob-join, mejora dicho código para que el programa devuelva\n",
    "cuántos clientes con valoración “bueno” hay en cada país. En concreto, la salida\n",
    "del programa MapReduce debe ser un fichero con el contenido que se muestra\n",
    "en la Figura 1 . Este ejercicio tiene una puntuación de hasta 3 puntos sobre 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diseño del programa MapReduce\n",
    "#### Limpieza de los datos\n",
    "En la primera parte del ejercicio se observan los documentos countries.csv y clientes.csv.\n",
    "En particular se observa que el documento countries.csv tiene cabezera (Name,Code) y que algunos paises tiene una coma dentro de su nombre lo cual puede crear problemas a la hora de separar las columnas. por ejemplo: `\"Bolivia, Plurinational State of\"`\n",
    "\n",
    "Se elimina la cabezera del documento y se limpian las columnas quitando la coma y el entrecomillado:\n",
    "\n",
    "`\"Bolivia, Plurinational State of\" -> Plurinational State of Bolivia`\n",
    "#### Diseño MapReduce\n",
    "El ejercicio se ha planteado usando un paso Map y un paso Reduce.\n",
    "- Map. El map se usa para leer los dos documentos y extraer la información necesaria.\n",
    "Se necesita extraer la columna nombre del pais del documentro countries y la columna valoración == 'bueno' del documento clients. Las dos tablas se encuentran enlazadas por la columna country2digit.\n",
    "\n",
    "Para el documento countries se separa en dos columnas, nombre del pais y country2digit. Se envia como clave la columna country2digit y como valor una lista formada por la letra 'A' y el nombre del pais.\n",
    "\n",
    "Para el documento clients se separa en tres columnas, nombre del cliente, valoración y country2digit. Aquí es importante que filtremos las columnas en las que la valoración es 'bueno'. Enviamos como clave country2digit, que es la columna que nos sirve para relacionar las dos tablas, y como valor enviamos una lista formada por la letra 'B' y como segundo elemento el número 1. Indicando que para ese pais hemos encontrado al menos 1 valoración buena.\n",
    "\n",
    "Se utiliza las etiquetas 'A' y 'B' junto con la opción: \n",
    "```python \n",
    "SORT_VALUES = True```\n",
    "Para que al reducer los valores llegen ordenados. Como sabemos, habrá un reducer para cada country2digit, es decir para cada país. Y los valores vendrán ordenados primero con la etiqueta 'A', solo habrá una y corresponde al documento countries con el nombre completo del pais, y luego todos los valores con la etiqueta 'B', procedentes del documento clients indicando que se ha encontrado una valoración buena.\n",
    "\n",
    "- Reduce. La función reducer va a contar el numero de valoraciones positivas que le llega de cada pais.\n",
    "\n",
    "Por tanto para cada country2digit primero recibirá una lista con la etiqueda 'A' y el nombre completo del pais. Luego recibirá un número de valores con la etiqueta 'B'. El reducer va sumando cuantas 'B' le llegan correspodiendose cada una con una valoración buena.\n",
    "\n",
    "Para cada country2digit devuelve una tuple con dos valores el nombre del pais y la suma de las etiquetas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/tp1/ejercicio101\r\n"
     ]
    }
   ],
   "source": [
    "# Se crea la carpeta ejercicio101 donde se van a almacenar los documentos que se generen\n",
    "! mkdir -p tp1/ejercicio101\n",
    "import os\n",
    "os.chdir(\"/media/notebooks/tp1/ejercicio101\")\n",
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los ficheros countries.csv y clients.csv deben estar descargados en la carpeta /media/notebooks/tp1/ejercicio101\n",
    "! cp /media/notebooks/countries.csv /media/notebooks/tp1/ejercicio101/\n",
    "! cp /media/notebooks/clients.csv /media/notebooks/tp1/ejercicio101/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name,Code\r",
      "\r\n",
      "Afghanistan,AF\r",
      "\r\n",
      "Åland Islands,AX\r",
      "\r\n",
      "Albania,AL\r",
      "\r\n",
      "Algeria,DZ\r",
      "\r\n",
      "American Samoa,AS\r",
      "\r\n",
      "Andorra,AD\r",
      "\r\n",
      "Angola,AO\r",
      "\r\n",
      "Anguilla,AI\r",
      "\r\n",
      "Antarctica,AQ\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! head countries.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Bolivia, Plurinational State of\",BO\n",
      "\n",
      "Plurinational State of Bolivia,BO\n",
      "\n",
      "\"Bonaire, Sint Eustatius and Saba\",BQ\n",
      "\n",
      "Sint Eustatius and Saba Bonaire,BQ\n",
      "\n",
      "\"Congo, the Democratic Republic of the\",CD\n",
      "\n",
      "the Democratic Republic of the Congo,CD\n",
      "\n",
      "\"Iran, Islamic Republic of\",IR\n",
      "\n",
      "Islamic Republic of Iran,IR\n",
      "\n",
      "\"Korea, Democratic People's Republic of\",KP\n",
      "\n",
      "Democratic People's Republic of Korea,KP\n",
      "\n",
      "\"Korea, Republic of\",KR\n",
      "\n",
      "Republic of Korea,KR\n",
      "\n",
      "\"Macedonia, the Former Yugoslav Republic of\",MK\n",
      "\n",
      "the Former Yugoslav Republic of Macedonia,MK\n",
      "\n",
      "\"Micronesia, Federated States of\",FM\n",
      "\n",
      "Federated States of Micronesia,FM\n",
      "\n",
      "\"Moldova, Republic of\",MD\n",
      "\n",
      "Republic of Moldova,MD\n",
      "\n",
      "\"Palestine, State of\",PS\n",
      "\n",
      "State of Palestine,PS\n",
      "\n",
      "\"Saint Helena, Ascension and Tristan da Cunha\",SH\n",
      "\n",
      "Ascension and Tristan da Cunha Saint Helena,SH\n",
      "\n",
      "\"Taiwan, Province of China\",TW\n",
      "\n",
      "Province of China Taiwan,TW\n",
      "\n",
      "\"Tanzania, United Republic of\",TZ\n",
      "\n",
      "United Republic of Tanzania,TZ\n",
      "\n",
      "\"Venezuela, Bolivarian Republic of\",VE\n",
      "\n",
      "Bolivarian Republic of Venezuela,VE\n",
      "\n",
      "\"Virgin Islands, British\",VG\n",
      "\n",
      "British Virgin Islands,VG\n",
      "\n",
      "\"Virgin Islands, U.S.\",VI\n",
      "\n",
      "U.S. Virgin Islands,VI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Limpieza de countries.csv\n",
    "\n",
    "with open(\"countries.csv\",'r') as f, open(\"countries_cleaned.csv\",'w') as f1:\n",
    "    next(f) # salta el encabezado\n",
    "    for line in f:\n",
    "        if '''\"''' in line: # Los paises con el problema de la coma estan entrecomillados\n",
    "            loc = line.find(',') # Elimina la primera coma para no confundir con la separacion entre columnas\n",
    "            new_line = line[loc+2:-5] + ' ' + line[1:loc] + line[-4:] # Reordena el nombre del pais\n",
    "            f1.write(new_line)\n",
    "            print(line)\n",
    "            print(new_line)\n",
    "        else:\n",
    "            f1.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afghanistan,AF\r\n",
      "Åland Islands,AX\r\n",
      "Albania,AL\r\n",
      "Algeria,DZ\r\n",
      "American Samoa,AS\r\n",
      "Andorra,AD\r\n",
      "Angola,AO\r\n",
      "Anguilla,AI\r\n",
      "Antarctica,AQ\r\n",
      "Antigua and Barbuda,AG\r\n"
     ]
    }
   ],
   "source": [
    "! head countries_cleaned.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bertram Pearcy  ,bueno,SO\r\n",
      "Steven Ulman  ,regular,ZA\r\n",
      "Enid Follansbee  ,malo,GS\r\n",
      "Candie Jacko  ,malo,SS\r\n",
      "Alana Zufelt  ,regular,ES\r\n",
      "Craig Pinkett  ,malo,LK\r\n",
      "Carson Levey  ,bueno,GU\r\n",
      "Reanna Calabrese  ,regular,GT\r\n",
      "Elliott Kosak  ,malo,GG\r\n",
      "Yuette Steinman  ,bueno,GN\r\n"
     ]
    }
   ],
   "source": [
    "! head clients.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob-ejercicio101.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob-ejercicio101.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "\n",
    "    SORT_VALUES = True\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        splits = line.rstrip(\"\\n\").split(\",\")\n",
    "        if len(splits) == 2: # datos de paises\n",
    "            symbol = 'A'     # diferencia datos de paises de clientes\n",
    "            country2digit = splits[1]\n",
    "            yield country2digit, [symbol, splits[0]] #valor es el simbolo A y el nombre del pais completo\n",
    "        else: #  datos de personas\n",
    "            symbol = 'B' \n",
    "            country2digit = splits[2]\n",
    "            if splits[1] == 'bueno': # seleccionamos solo los clientes con una valoracion buena\n",
    "                yield country2digit, [symbol, 1] # enviamos solo un valor cuando hemos encontrado una val buena \n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        for value in values:\n",
    "            if value[0] == 'A': # dato de pais\n",
    "                country = value[1] # nombre del pais\n",
    "                counter = 0\n",
    "            if value[0] == 'B': # dato de persona\n",
    "                counter +=1 # sumamos 1 por cada valoración buena\n",
    "        else:\n",
    "            if counter > 0: # solo enviamos resultados hay al menos 1 valoración buena\n",
    "                yield country, counter\n",
    "                \n",
    "if __name__ == '__main__':\n",
    "    MRJoin.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primero ejecutamos el código en local:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/mrjob-ejercicio101.root.20201030.165619.968030\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/mrjob-ejercicio101.root.20201030.165619.968030/output\n",
      "Streaming final output from /tmp/mrjob-ejercicio101.root.20201030.165619.968030/output...\n",
      "Removing temp directory /tmp/mrjob-ejercicio101.root.20201030.165619.968030...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio101.py /media/notebooks/tp1/ejercicio101/countries_cleaned.csv  \\\n",
    "/media/notebooks/tp1/ejercicio101/clients.csv > ouputlocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Spain\"\t3\r\n",
      "\"Guinea\"\t1\r\n",
      "\"South Georgia and the South Sandwich Islands\"\t1\r\n",
      "\"Guam\"\t3\r\n",
      "\"Canada\"\t1\r\n",
      "\"Somalia\"\t1\r\n",
      "\"South Sudan\"\t1\r\n",
      "\"Turkey\"\t1\r\n",
      "\"United States\"\t1\r\n",
      "\"South Africa\"\t1\r\n",
      "\"Portugal\"\t1\r\n",
      "\"Qatar\"\t1\r\n"
     ]
    }
   ],
   "source": [
    "! cat ouputlocal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecutamos el código en hadoop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/tmp/mrjoin/ejercicio101': File exists\n",
      "put: `/tmp/mrjoin/ejercicio101/countries_cleaned.csv': File exists\n",
      "put: `/tmp/mrjoin/ejercicio101/clients.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -mkdir /tmp/mrjoin/ejercicio101\n",
    "! hdfs dfs -put /media/notebooks/tp1/ejercicio101/countries_cleaned.csv  /tmp/mrjoin/ejercicio101\n",
    "! hdfs dfs -put /media/notebooks/tp1/ejercicio101/clients.csv  /tmp/mrjoin/ejercicio101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/carpeta/mrjob-ejercicio101/_SUCCESS\n",
      "Deleted /tmp/carpeta/mrjob-ejercicio101/part-00000\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm /tmp/carpeta/mrjob-ejercicio101/*\n",
    "! hdfs dfs -rmdir /tmp/carpeta/mrjob-ejercicio101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/lib/hadoop/bin...\n",
      "Found hadoop binary: /usr/lib/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/mrjob-ejercicio101.root.20201030.165632.519590\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/mrjob-ejercicio101.root.20201030.165632.519590/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/mrjob-ejercicio101.root.20201030.165632.519590/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob1409532757746871196.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.19.0.4:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.19.0.4:8032\n",
      "  Total input paths to process : 2\n",
      "  number of splits:3\n",
      "  Submitting tokens for job: job_1604063155650_0012\n",
      "  Submitted application application_1604063155650_0012\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1604063155650_0012/\n",
      "  Running job: job_1604063155650_0012\n",
      "  Job job_1604063155650_0012 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1604063155650_0012 completed successfully\n",
      "  Output directory: hdfs:///tmp/carpeta/mrjob-ejercicio101\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6363\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=187\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=7596\n",
      "\t\tFILE: Number of bytes written=610913\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6707\n",
      "\t\tHDFS: Number of bytes written=187\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=11838464\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2740224\n",
      "\t\tTotal time spent by all map tasks (ms)=11561\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11561\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2676\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2676\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=11561\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2676\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1290\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=382\n",
      "\t\tInput split bytes=344\n",
      "\t\tMap input records=299\n",
      "\t\tMap output bytes=7060\n",
      "\t\tMap output materialized bytes=7608\n",
      "\t\tMap output records=265\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tPhysical memory (bytes) snapshot=1094283264\n",
      "\t\tReduce input groups=261\n",
      "\t\tReduce input records=265\n",
      "\t\tReduce output records=12\n",
      "\t\tReduce shuffle bytes=7608\n",
      "\t\tShuffled Maps =3\n",
      "\t\tSpilled Records=530\n",
      "\t\tTotal committed heap usage (bytes)=1224212480\n",
      "\t\tVirtual memory (bytes) snapshot=10442010624\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///tmp/carpeta/mrjob-ejercicio101\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/mrjob-ejercicio101.root.20201030.165632.519590...\n",
      "Removing temp directory /tmp/mrjob-ejercicio101.root.20201030.165632.519590...\n"
     ]
    }
   ],
   "source": [
    "! python mrjob-ejercicio101.py hdfs:///tmp/mrjoin/ejercicio101/* -r hadoop --python-bin /opt/anaconda/bin/python3.7 \\\n",
    "--output-dir hdfs:///tmp/carpeta/mrjob-ejercicio101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Canada\"\t1\r\n",
      "\"Spain\"\t3\r\n",
      "\"Guinea\"\t1\r\n",
      "\"South Georgia and the South Sandwich Islands\"\t1\r\n",
      "\"Guam\"\t3\r\n",
      "\"Portugal\"\t1\r\n",
      "\"Qatar\"\t1\r\n",
      "\"Somalia\"\t1\r\n",
      "\"South Sudan\"\t1\r\n",
      "\"Turkey\"\t1\r\n",
      "\"United States\"\t1\r\n",
      "\"South Africa\"\t1\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -tail /tmp/carpeta/mrjob-ejercicio101/part-00000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
